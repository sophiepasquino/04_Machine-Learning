{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS COMMAND ONLY IF YOU USE GOOGLE COLAB.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS COMMAND ONLY IF YOU USE GOOGLE COLAB.\n",
    "%cd drive/MyDrive/TechLabs/04_Machine\\ Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17751b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THESE FIRST.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.datasets import (make_classification, make_blobs, load_breast_cancer, make_regression, make_friedman1,\n",
    "                              load_iris)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, train_test_split\n",
    "from data.adspy_shared_utilities import (load_crime_dataset, plot_two_class_knn, \n",
    "                                         plot_class_regions_for_classifier_subplot, \n",
    "                                         plot_class_regions_for_classifier,\n",
    "                                         plot_feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85ac7",
   "metadata": {},
   "source": [
    "# Chapter 02 - Supervised Machine Learning\n",
    "### Hey Techie,   \n",
    "Welcome to the second notebook of this Machine Learning tutorial series. Today's videos introduce you to the concepts and different algorithms of supervised machine learning.   \n",
    "This notebook is designed to allow you to take notes as you watch each video and learn along the code discussed in the videos. At the end of the notebook, you will find practice tasks that you can solve on your own and compare to our sample solution.   \n",
    "After auditing the course, you may find the respective materials here: https://www.coursera.org/learn/python-machine-learning/home/week/2\n",
    "#### Have fun! :-)   \n",
    "*Video length in total*: 160 minutes   \n",
    "*Self-study time*: 160 minutes   \n",
    "*Total*: **320 minutes**   \n",
    "#### Credits\n",
    "Applied Machine Learning in Python, University of Michigan (Coursera), https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python\n",
    "<hr style=\"border:2px solid gray\"> </hr>   \n",
    "   \n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827a991",
   "metadata": {},
   "source": [
    "* Take notes here.\n",
    "    * And here.\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f49f8d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>   \n",
    "   \n",
    "## Applied Machine Learning, Module 2:  Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b556a2c",
   "metadata": {},
   "source": [
    "### Preamble and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "fruits = pd.read_table('data/fruit_data_with_colors.txt')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "example_fruit = [[5.5, 2.2, 10, 0.70]]\n",
    "example_fruit_scaled = scaler.transform(example_fruit)\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "          target_names_fruits[knn.predict(example_fruit_scaled)[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376975d2",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf3a75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# synthetic dataset for more complex regression\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "# synthetic dataset for classification (binary) \n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# more difficult synthetic dataset for classification (binary) \n",
    "# with classes that are not linearly separable\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Breast cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "\n",
    "# Communities and Crime dataset\n",
    "(X_crime, y_crime) = load_crime_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d8ae1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81401f04",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007efbf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c594129",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\n",
    "\n",
    "print(knnreg.predict(X_test))\n",
    "print('R-squared test score: {:.3f}'\n",
    "     .format(knnreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 2, figsize=(8,4))\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    thisaxis.set_xlim([-2.5, 0.75])\n",
    "    thisaxis.plot(X_predict_input, y_predict_output, '^', markersize = 10,\n",
    "                 label='Predicted', alpha=0.8)\n",
    "    thisaxis.plot(X_train, y_train, 'o', label='True Value', alpha=0.8)\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN regression (K={})'.format(K))\n",
    "    thisaxis.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbd7f0",
   "metadata": {},
   "source": [
    "#### Regression model complexity as a function of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222eea5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot k-NN regression on sample dataset for different values of K\n",
    "fig, subaxes = plt.subplots(5, 1, figsize=(5,20))\n",
    "X_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3, 7, 15, 55]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    train_score = knnreg.score(X_train, y_train)\n",
    "    test_score = knnreg.score(X_test, y_test)\n",
    "    thisaxis.plot(X_predict_input, y_predict_output)\n",
    "    thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n",
    "    thisaxis.plot(X_test, y_test, '^', alpha=0.9, label='Test')\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN Regression (K={})\\n\\\n",
    "Train $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n",
    "                      .format(K, train_score, test_score))\n",
    "    thisaxis.legend()\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3304527",
   "metadata": {},
   "source": [
    "### Linear models for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa6ade",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c40bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e848827",
   "metadata": {},
   "source": [
    "#### Linear regression: example plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae84539",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d207e8",
   "metadata": {},
   "source": [
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d281dc",
   "metadata": {},
   "source": [
    "##### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e43578",
   "metadata": {},
   "source": [
    "##### Ridge regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dae7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c55aef",
   "metadata": {},
   "source": [
    "#### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf50f0d",
   "metadata": {},
   "source": [
    "##### Lasso regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242ff73",
   "metadata": {},
   "source": [
    "#### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb130f75",
   "metadata": {},
   "source": [
    "### Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6c9e5",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179fb7a",
   "metadata": {},
   "source": [
    "##### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.to_numpy(),\n",
    "                y_fruits_apple.to_numpy(),\n",
    "                random_state = 0))\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
    "                                         None, 'Logistic regression \\\n",
    "for binary classification\\nFruit dataset: Apple vs others',\n",
    "                                         subaxes)\n",
    "\n",
    "h = 6\n",
    "w = 8\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][int(clf.predict([[h,w]])[0])]))\n",
    "\n",
    "h = 10\n",
    "w = 7\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][int(clf.predict([[h,w]])[0])]))\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width')\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8e04b",
   "metadata": {},
   "source": [
    "##### Logistic regression on simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00339d67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                         None, None, title, subaxes)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b24434",
   "metadata": {},
   "source": [
    "##### Logistic regression regularization: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72299e6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.to_numpy(),\n",
    "                y_fruits_apple.to_numpy(),\n",
    "                random_state=0))\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 10))\n",
    "\n",
    "for this_C, subplot in zip([0.1, 1, 100], subaxes):\n",
    "    clf = LogisticRegression(C=this_C).fit(X_train, y_train)\n",
    "    title ='Logistic regression (apple vs rest), C = {:.3f}'.format(this_C)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             X_test, y_test, title,\n",
    "                                             subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12a0e9",
   "metadata": {},
   "source": [
    "##### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c02238",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccdb37",
   "metadata": {},
   "source": [
    "##### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3377f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "this_C = 1.0\n",
    "clf = SVC(kernel='linear', C=this_C).fit(X_train, y_train)\n",
    "title = 'Linear SVC, C = {:.3f}'.format(this_C)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490947c5",
   "metadata": {},
   "source": [
    "##### Linear Support Vector Machine: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637594a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for this_C, subplot in zip([0.00001, 100], subaxes):\n",
    "    clf = SVC(kernel='linear', C=this_C).fit(X_train, y_train)\n",
    "    title = 'Linear SVC, C = {:.5f}'.format(this_C)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e24d4",
   "metadata": {},
   "source": [
    "##### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ce2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e1787",
   "metadata": {},
   "source": [
    "#### Multi-class classification with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29247b",
   "metadata": {},
   "source": [
    "##### Linear SVC with M classes generates M one vs rest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
    "\n",
    "clf = SVC(kernel='linear', C=5, random_state = 67).fit(X_train, y_train)\n",
    "print('Coefficients:\\n', clf.coef_)\n",
    "print('Intercepts:\\n', clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd62458",
   "metadata": {},
   "source": [
    "##### Multi-class results on the fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
    "\n",
    "plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n",
    "           c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n",
    "\n",
    "x_0_range = np.linspace(-10, 15)\n",
    "\n",
    "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
    "    # Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b, \n",
    "    # and the decision boundary is defined as being all points with y = 0, to plot x_1 as a \n",
    "    # function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:\n",
    "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], c=color, alpha=.8)\n",
    "    \n",
    "plt.legend(target_names_fruits)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('width')\n",
    "plt.xlim(-2, 12)\n",
    "plt.ylim(-2, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee695688",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0d435",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507efb1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "\n",
    "# The default SVC kernel is radial basis function (RBF)\n",
    "plot_class_regions_for_classifier(SVC().fit(X_train, y_train),\n",
    "                                 X_train, y_train, None, None,\n",
    "                                 'Support Vector Classifier: RBF kernel')\n",
    "\n",
    "# Compare decision boundries with polynomial kernel, degree = 3\n",
    "plot_class_regions_for_classifier(SVC(kernel = 'poly', degree = 3)\n",
    "                                 .fit(X_train, y_train), X_train,\n",
    "                                 y_train, None, None,\n",
    "                                 'Support Vector Classifier: Polynomial kernel, degree = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303dcdf",
   "metadata": {},
   "source": [
    "##### Support Vector Machine with RBF kernel: gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d426be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n",
    "\n",
    "for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n",
    "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n",
    "    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7277290",
   "metadata": {},
   "source": [
    "##### Support Vector Machine with RBF kernel: using both C and gamma parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 4, figsize=(15, 10), dpi=50)\n",
    "\n",
    "for this_gamma, this_axis in zip([0.01, 1, 5], subaxes):\n",
    "    \n",
    "    for this_C, subplot in zip([0.1, 1, 15, 250], this_axis):\n",
    "        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n",
    "        clf = SVC(kernel = 'rbf', gamma = this_gamma,\n",
    "                 C = this_C).fit(X_train, y_train)\n",
    "        plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                                 X_test, y_test, title,\n",
    "                                                 subplot)\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4dde2",
   "metadata": {},
   "source": [
    "#### Application of SVMs to a real dataset: unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6830c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train, y_train)\n",
    "print('Breast cancer dataset (unnormalized features)')\n",
    "print('Accuracy of RBF-kernel SVC on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RBF-kernel SVC on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b1a30",
   "metadata": {},
   "source": [
    "#### Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294dd6ad",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7668a0",
   "metadata": {},
   "source": [
    "#### Example based on k-NN classifier with fruit dataset (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "X = X_fruits_2d.to_numpy()\n",
    "y = y_fruits_2d.to_numpy()\n",
    "cv_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "print('Cross-validation scores (3-fold):', cv_scores)\n",
    "print('Mean cross-validation score (3-fold): {:.3f}'\n",
    "     .format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630d46f",
   "metadata": {},
   "source": [
    "#### A note on performing cross-validation for more advanced scenarios.\n",
    "\n",
    "In some cases (e.g. when feature values have very different ranges), we've seen the need to scale or normalize the training and test sets before use with a classifier. The proper way to do cross-validation when you need to scale the data is *not* to scale the entire dataset with a single transform, since this will indirectly leak information into the training data about the whole dataset, including the test data (see the lecture on data leakage later in the course).  Instead, scaling/normalizing must be computed and applied for each cross-validation fold separately.  To do this, the easiest way in scikit-learn is to use *pipelines*.  While these are beyond the scope of this course, further information is available in the scikit-learn documentation here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "or the Pipeline section in the recommended textbook: Introduction to Machine Learning with Python by Andreas C. MÃ¼ller and Sarah Guido (O'Reilly Media)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0f91",
   "metadata": {},
   "source": [
    "### Validation curve example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = np.logspace(-3, 3, 4)\n",
    "train_scores, test_scores = validation_curve(SVC(), X, y,\n",
    "                                            param_name='gamma',\n",
    "                                            param_range=param_range, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d917807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba28c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "# See: http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with SVM')\n",
    "plt.xlabel('$\\gamma$ (gamma)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdcd0c",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99641ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e3328",
   "metadata": {},
   "source": [
    "##### Setting max decision tree depth to help avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68e3b4",
   "metadata": {},
   "source": [
    "##### Visualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd628e85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=150)\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6eadde",
   "metadata": {},
   "source": [
    "##### Pre-pruned version (max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd150c8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=150)\n",
    "plot_tree(clf2, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bf105",
   "metadata": {},
   "source": [
    "##### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3aa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,4), dpi=100)\n",
    "plot_feature_importances(clf, iris.feature_names)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86da443",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 0)\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
    "\n",
    "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
    "tree_max_depth = 4\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n",
    "    title = 'Decision Tree, max_depth = {:d}'.format(tree_max_depth)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
    "                                             None, title, axis,\n",
    "                                             iris.target_names)\n",
    "    \n",
    "    axis.set_xlabel(iris.feature_names[pair[0]])\n",
    "    axis.set_ylabel(iris.feature_names[pair[1]])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5b25b",
   "metadata": {},
   "source": [
    "##### Decision Trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fc25f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
    "                            random_state = 0).fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=150)\n",
    "\n",
    "plot_tree(clf,feature_names = cancer.feature_names, class_names=cancer.target_names, filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Breast cancer dataset: decision tree')\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "plt.figure(figsize=(10,6),dpi=150)\n",
    "plot_feature_importances(clf, cancer.feature_names)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918ea20",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>   \n",
    "   \n",
    "## Practice Tasks   \n",
    "In these practice tasks, you'll explore the relationship between model complexity and generalization performance, by adjusting key parameters of various supervised learning models. Part 1 of this assignment will look at regression and Part 2 will look at classification.   \n",
    "## Part 1 - Regression   \n",
    "First, run the following block to set up the variables needed for later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 15\n",
    "x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# You can use this function to help you visualize the dataset by\n",
    "# plotting a scatterplot of the data points\n",
    "# in the training and test sets.\n",
    "def part1_scatter():\n",
    "    plt.figure()\n",
    "    plt.scatter(X_train, y_train, label='training data')\n",
    "    plt.scatter(X_test, y_test, label='test data')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "part1_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077a30b",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Write a function that fits a polynomial LinearRegression model on the *training data* `X_train` for degrees 1, 3, 6, and 9. (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. `np.linspace(0,10,100)`) and store this in a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second row degree 3, the third row degree 6, and the fourth row degree 9.\n",
    "\n",
    "<img src=\"data/polynomialreg1.png\" style=\"width: 1000px;\"/>\n",
    "\n",
    "The figure above shows the fitted models plotted on top of the original data (using `plot_one()`).  \n",
    "   \n",
    "*This function should return a numpy array with shape `(4, 100)`* \n",
    "   \n",
    "<br /> \n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>Reshape your x-data in the desired format.</li>\n",
    "        <li>Write your code in a loop that iterates to degrees 1, 3, 6, and 9.</li>\n",
    "        <li>Within every iteration, add polynomial features to your x-data, fit the linear regression to the training data, predict the interval data, and store those predictions.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    # START YOUR CODE HERE.\n",
    "\n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9787da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feel free to use the function plot_one() to replicate the figure \n",
    "# from the prompt once you have completed question one\n",
    "def plot_one(degree_predictions):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "    plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "    for i,degree in enumerate([1,3,6,9]):\n",
    "        plt.plot(np.linspace(0,10,100), degree_predictions[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "    plt.ylim(-1,2.5)\n",
    "    plt.legend(loc=4)\n",
    "    plt.show();    \n",
    "    return \n",
    "\n",
    "plot_one(answer_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c58049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert answer_one().shape == (4, 100), \"Generate predictions for degrees 1, 3, 6, and 9!\"\n",
    "assert answer_one()[0][12] == 0.4469764174950076, \"Your predictions seem to be incorrect!\"\n",
    "assert answer_one()[1][26] == 0.3634138446485071, \"Your predictions seem to be incorrect!\"\n",
    "assert answer_one()[2][67] == 1.4450552552298015, \"Your predictions seem to be incorrect!\"\n",
    "assert answer_one()[3][85] == 2.162992885565613, \"Your predictions seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860aba12",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_one():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.linear_model import LinearRegression</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.preprocessing import PolynomialFeatures</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;results = None</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train_reshaped = X_train.reshape(-1, 1)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_interval = np.linspace(0, 10, 100).reshape(-1, 1)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;for degree in [1, 3, 6, 9]:</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;polynomial = PolynomialFeatures(degree=degree).fit(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_polynomial = polynomial.transform(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;interval_polynomial = polynomial.transform(X_interval)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model = LinearRegression().fit(X_polynomial, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;predictions = model.predict(interval_polynomial)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if degree == 1:</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results = predictions</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results = np.vstack((results, predictions))</code><br />\n",
    "    <code></code><br />\n",
    "    <code>    return results</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 2\n",
    "\n",
    "Write a function that fits a polynomial LinearRegression model on the training data `X_train` for degrees 0 through 9. For each model compute the $R^2$ (coefficient of determination) regression score on the training data as well as the the test data, and return both of these arrays in a tuple.\n",
    "\n",
    "*This function should return one tuple of numpy arrays `(r2_train, r2_test)`. Both arrays should have shape `(10,)`*\n",
    "   \n",
    "<br /> \n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>Reshape your x-data in the desired format.</li>\n",
    "        <li>Write your code in a loop that iterates through degrees 0 to 9.</li> \n",
    "        <li>Within every iteration, add polynomial features to your x-data, fit the linear regression to the training data, compute the r2-score for both the training and testing data, and store those scores separately.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e716d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    # START YOUR CODE HERE.\n",
    "        \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert type(answer_two()[0]) == type(np.array([])), \"Please return a tuple of numpy arrays!\" \n",
    "assert type(answer_two()[1]) == type(np.array([])), \"Please return a tuple of numpy arrays!\"\n",
    "assert answer_two()[0].shape == (10,), \"Please fit polynomial models for degrees 0 to 9!\"\n",
    "assert answer_two()[0].shape == (10,), \"Please fit polynomial models for degrees 0 to 9!\"\n",
    "assert answer_two()[0][-2] == 0.9963754538775279, \"Your r2 scores for the train set seem to be incorrect!\"\n",
    "assert answer_two()[1][1] == -0.45237104233936676, \"Your r2 scores for the test set seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506476b",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_two():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.linear_model import LinearRegression</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.preprocessing import PolynomialFeatures</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;r2_train = np.array([])</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;r2_test = np.array([])</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train_reshaped = X_train.reshape(-1, 1)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_test_reshaped = X_test.reshape(-1, 1)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;for degree in range(10):</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;polynomial = PolynomialFeatures(degree=degree).fit(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_train_polynomial = polynomial.transform(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_test_polynomial = polynomial.transform(X_test_reshaped)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model = LinearRegression().fit(X_train_polynomial, y_train)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r2_train = np.append(r2_train, model.score(X_train_polynomial, y_train))</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r2_test = np.append(r2_test, model.score(X_test_polynomial, y_test))</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return r2_train, r2_test</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 3\n",
    "\n",
    "Based on the $R^2$ scores from question 2 (degree levels 0 through 9), what degree level corresponds to a model that is underfitting? What degree level corresponds to a model that is overfitting? What choice of degree level would provide a model with good generalization performance on this dataset? \n",
    "\n",
    "Hint: Try plotting the $R^2$ scores from question 2 to visualize the relationship between degree level and $R^2$.\n",
    "\n",
    "*This function should return one tuple with the degree values in this order: `(Underfitting, Overfitting, Good_Generalization)`. There might be multiple correct solutions, however, you only need to return one possible solution, for example, (1,2,3).*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert sum(answer_three()) == 15 or sum(answer_three()) == 16, \"Your answer seems to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a1d8a",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    Solution 1:<br />\n",
    "    <code>def answer_three():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return (0, 9, 6)</code><br />\n",
    "    Solution 2:<br />\n",
    "    <code>def answer_three():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return (0, 9, 7)</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 4\n",
    "Training models on high degree polynomial features can result in overly complex models that overfit, so we often use regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.\n",
    "\n",
    "For this question, train two models: a non-regularized LinearRegression model (default parameters) and a regularized Lasso Regression model (with parameters `alpha=0.0001`, `max_iter=1000000`) both on polynomial features of degree 12. Before fitting the models, apply feature scaling using the MinMaxScaler. Return the $R^2$ score for both the LinearRegression and Lasso model's test sets.\n",
    "\n",
    "*This function should return one tuple `(LinearRegression_R2_test_score, Lasso_R2_test_score)`*  \n",
    "   \n",
    "<br /> \n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>Reshape your x-data in the desired format.</li>\n",
    "        <li>Add polynomial features to your x-data.</li>\n",
    "        <li>Scale your x-data.</li>\n",
    "        <li>Fit a model to the training data for both linear and lasso regression.</li>\n",
    "        <li>Return the RÂ²-score for both models relying on the testing data.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c49af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler                  \n",
    "    from sklearn.linear_model import Lasso, LinearRegression\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2182a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert len(answer_four()) == 2, \"Fit a model for both Linear and Lasso regression!\"\n",
    "assert sum(answer_four()) == -16.059014491675377, \"Your RÂ² scores seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ffd370",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_four():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.linear_model import Lasso, LinearRegression</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train_reshaped = X_train.reshape(-1, 1)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_test_reshaped = X_test.reshape(-1, 1)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;polynomial = PolynomialFeatures(degree=12).fit(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train_polynomial = polynomial.transform(X_train_reshaped)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_test_polynomial = polynomial.transform(X_test_reshaped)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;scaler = MinMaxScaler().fit(X_train_polynomial)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train_scaled = scaler.transform(X_train_polynomial)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_test_scaled = scaler.transform(X_test_polynomial)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;linear_model = LinearRegression().fit(X_train_scaled, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;lasso_model = Lasso(alpha=0.0001, max_iter=1000000).fit(X_train_scaled, y_train)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return linear_model.score(X_test_scaled, y_test), lasso_model.score(X_test_scaled, y_test)</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "## Part 2 - Classification\n",
    "\n",
    "Here's an application of machine learning that could save your life! For this section of the assignment we will be working with the [UCI Mushroom Data Set](http://archive.ics.uci.edu/ml/datasets/Mushroom?ref=datanews.io) stored in `data/mushrooms.csv`. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:\n",
    "\n",
    "*Attribute Information:*\n",
    "\n",
    "1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \n",
    "2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s \n",
    "3. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "4. bruises?: bruises=t, no=f \n",
    "5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s \n",
    "6. gill-attachment: attached=a, descending=d, free=f, notched=n \n",
    "7. gill-spacing: close=c, crowded=w, distant=d \n",
    "8. gill-size: broad=b, narrow=n \n",
    "9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "10. stalk-shape: enlarging=e, tapering=t \n",
    "11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=? \n",
    "12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "16. veil-type: partial=p, universal=u \n",
    "17. veil-color: brown=n, orange=o, white=w, yellow=y \n",
    "18. ring-number: none=n, one=o, two=t \n",
    "19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z \n",
    "20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y \n",
    "21. population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y \n",
    "22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
    "\n",
    "<br>\n",
    "\n",
    "The data in the mushrooms dataset is currently encoded with strings. These values will need to be encoded to numeric to work with sklearn. We'll use pd.get_dummies to convert the categorical variables into indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mush_df = pd.read_csv('data/mushrooms.csv')\n",
    "mush_df2 = pd.get_dummies(mush_df)\n",
    "\n",
    "X_mush = mush_df2.iloc[:,2:]\n",
    "y_mush = mush_df2.iloc[:,1]\n",
    "\n",
    "# Use the variables X_train_mush and y_train_mush for task 5.\n",
    "X_train_mush, X_test_mush, y_train_mush, y_test_mush = train_test_split(X_mush, y_mush, random_state=0)\n",
    "\n",
    "# For performance reasons in task 6, we will create a smaller version of the entire\n",
    "# mushroom dataset for use in this question. For simplicity, we'll just re-use the \n",
    "# 25% test split created above as the representative subset.\n",
    "# Therefore, use the variables X_subset and y_subset for task 6.\n",
    "X_subset = X_test_mush\n",
    "y_subset = y_test_mush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a410b88",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Using `X_train_mush` and `y_train_mush` from the preceeding cell, train a DecisionTreeClassifier with default parameters and `random_state=0`. What are the 5 most important features found by the decision tree?\n",
    "\n",
    "As a reminder, the feature names are available in the `X_train_mush.columns` property, and the order of the features in `X_train_mush.columns` matches the order of the feature importance values in the classifier's `feature_importances_` property. \n",
    "\n",
    "*This function should return a list of length 5 containing the feature names in descending order of importance.*\n",
    "\n",
    "*Note: Remember that you also need to set random_state in the DecisionTreeClassifier.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>Remind yourself of sequence functions (Introduction to Python Programming; Chapter 10).</li>\n",
    "        <li>Zip the feature importances and names in one list.</li>\n",
    "        <li>Sort this list by the feature importances in descending order.</li>\n",
    "        <li>Return the first five feature names from the sorted list.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f69d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert len(answer_five()) == 5, \"Please return a list of length 5!\"\n",
    "assert answer_five()[0] == \"odor_n\", \"Your feature list seem to be incorrect!\"\n",
    "assert answer_five()[-1] == \"odor_l\", \"Your feature list seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d833926",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_five():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.tree import DecisionTreeClassifier</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;model = DecisionTreeClassifier(random_state=0).fit(X_train_mush, y_train_mush)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;merge = sorted([(importance, feature_name) for importance, feature_name in </code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;zip(model.feature_importances_, X_train_mush.columns)], key=lambda x: -x[0])</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return [item[1] for item in merge[:5]]</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 6   \n",
    "\n",
    "For this question, we're going to use the `validation_curve` function in `sklearn.model_selection` to determine training and test scores for a Support Vector Classifier (`SVC`) with varying parameter values.  Recall that the validation_curve function, in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal train-test splits to compute results.\n",
    "\n",
    "**Because creating a validation curve requires fitting multiple models, for performance reasons this question will use just a subset of the original mushroom dataset: please use the variables X_subset and y_subset as input to the validation curve function (instead of X_mush and y_mush) to reduce computation time.**\n",
    "\n",
    "The initialized unfitted classifier object we'll be using is a Support Vector Classifier with radial basis kernel.  So your first step is to create an `SVC` object with default parameters (i.e., `kernel='rbf', C=1`). Recall that the kernel width of the RBF kernel is controlled using the `gamma` parameter.  \n",
    "\n",
    "With this classifier, and the dataset in X_subset, y_subset, explore the effect of `gamma` on classifier accuracy by using the `validation_curve` function to find the training and test scores for 6 values of `gamma` from `0.0001` to `10` (i.e., `np.logspace(-4,1,6)`). Recall that you can specify what scoring metric you want  `validation_curve` to use by setting the `scoring` parameter.  In this case, we want to use \"accuracy\" as the scoring metric.\n",
    "\n",
    "For each level of `gamma`, `validation_curve` will fit 5 models on different subsets of the data, returning two 6x5 (6 levels of gamma x 5 fits per level) arrays of the scores for the training and test sets.\n",
    "\n",
    "Find the mean score across the five models for each level of `gamma` for both arrays, creating two arrays of length 6, and return a tuple with the two arrays.\n",
    "\n",
    "e.g.\n",
    "\n",
    "if one of your array of scores is\n",
    "\n",
    "    array([[ 0.5,  0.4,  0.6],\n",
    "           [ 0.7,  0.8,  0.7],\n",
    "           [ 0.9,  0.8,  0.8],\n",
    "           [ 0.8,  0.7,  0.8],\n",
    "           [ 0.7,  0.6,  0.6],\n",
    "           [ 0.4,  0.6,  0.5]])\n",
    "       \n",
    "it should then become\n",
    "\n",
    "    array([ 0.5,  0.73333333,  0.83333333,  0.76666667,  0.63333333, 0.5])\n",
    "\n",
    "*This function should return one tuple of numpy arrays `(training_scores, test_scores)` where each array in the tuple has shape `(6,)`.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>The validation_curve-method returns a tuple containing two numpy-arrays referring to training- and test-set scores.</li>\n",
    "        <li>You need to set the following parameters for the validation_curve-method call: estimator, X, y, param_name, param_range, and scoring.</li> \n",
    "        <li>Using numpy, one can calculate row- or column-wise means if the input is a matrix (axis parameter!).</li> \n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import validation_curve\n",
    "    import numpy as np\n",
    "    #START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "results = answer_six()\n",
    "assert type(results) == type(tuple()), \"Please return a tuple of length two!\"\n",
    "assert results[0].shape == (6,) and results[1].shape == (6,), \"The returned arrays are in the wrong format!\"\n",
    "assert sum(results[0]) == 5.76415816597196, \"Your results seem to be incorrect!\"\n",
    "assert sum(results[1]) == 5.280157587054139, \"Your results seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35044d",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_six():</code><br/>\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;X_train, X_test, y_train, y_test = answer_five()</code><br/>\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return KNeighborsClassifier(n_neighbors = 1).fit(X_train, y_train)</code><br/> \n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
