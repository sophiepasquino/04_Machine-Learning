{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c306a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS COMMAND ONLY IF YOU USE GOOGLE COLAB.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS COMMAND ONLY IF YOU USE GOOGLE COLAB.\n",
    "%cd drive/MyDrive/TechLabs/04_Machine\\ Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17751b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THESE FIRST.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             classification_report, precision_recall_curve, roc_curve, auc, mean_squared_error,\n",
    "                             r2_score, roc_auc_score)\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from data.adspy_shared_utilities import (plot_class_regions_for_classifier_subplot, \n",
    "                                         plot_class_regions_for_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85ac7",
   "metadata": {},
   "source": [
    "# Chapter 03 - Evaluation\n",
    "### Hey Techie,   \n",
    "Welcome to the third notebook of this Machine Learning tutorial series. Today's videos introduce you to the concepts of model evaluation for both classification and regression.   \n",
    "This notebook is designed to allow you to take notes as you watch each video and learn along the code discussed in the videos. At the end of the notebook, you will find practice tasks that you can solve on your own and compare to our sample solution.   \n",
    "After auditing the course, you may find the respective materials here: https://www.coursera.org/learn/python-machine-learning/home/week/3\n",
    "#### Have fun! :-)   \n",
    "*Video length in total*: 80 minutes   \n",
    "*Self-study time*: 80 minutes   \n",
    "*Total*: **160 minutes**   \n",
    "#### Credits\n",
    "Applied Machine Learning in Python, University of Michigan (Coursera), https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python\n",
    "<hr style=\"border:2px solid gray\"> </hr>   \n",
    "   \n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827a991",
   "metadata": {},
   "source": [
    "* Take notes here.\n",
    "    * And here.\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f49f8d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>   \n",
    "   \n",
    "## Applied Machine Learning, Module 3:  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eca54e",
   "metadata": {},
   "source": [
    "## Evaluation for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f0ec6",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name,class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa68374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset with imbalanced binary classes:  \n",
    "# Negative class (0) is 'not digit 1' \n",
    "# Positive class (1) is 'digit 1'\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print('Original labels:\\t', y[1:30])\n",
    "print('New binary labels:\\t', y_binary_imbalanced[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60238768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.bincount(y_binary_imbalanced)    # Negative class (0) is the most frequent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4894d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "# Accuracy of Support Vector Machine classifier\n",
    "svm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42935b",
   "metadata": {},
   "source": [
    "### Dummy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb18ba",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DummyClassifier is a classifier that makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers, especially with imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5846cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "y_dummy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_majority.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37209ba6",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7634f25",
   "metadata": {},
   "source": [
    "#### Binary (two-class) confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5461c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "y_majority_predicted = dummy_majority.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_majority_predicted)\n",
    "\n",
    "print('Most frequent class (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces random predictions w/ same class proportion as training set\n",
    "dummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_classprop_predicted)\n",
    "\n",
    "print('Random class-proportional prediction (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd41eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "svm_predicted = svm.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, svm_predicted)\n",
    "\n",
    "print('Support vector machine classifier (linear kernel, C=1)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "lr_predicted = lr.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "tree_predicted = dt.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, tree_predicted)\n",
    "\n",
    "print('Decision tree classifier (max_depth = 2)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13432f1a",
   "metadata": {},
   "source": [
    "### Evaluation metrics for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined report with all above metrics\n",
    "print(classification_report(y_test, tree_predicted, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3f55f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Random class-proportional (dummy)\\n', \n",
    "      classification_report(y_test, y_classprop_predicted, target_names=['not 1', '1']))\n",
    "print('SVM\\n', \n",
    "      classification_report(y_test, svm_predicted, target_names = ['not 1', '1']))\n",
    "print('Logistic regression\\n', \n",
    "      classification_report(y_test, lr_predicted, target_names = ['not 1', '1']))\n",
    "print('Decision tree\\n', \n",
    "      classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18be49",
   "metadata": {},
   "source": [
    "### Decision functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8918f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "# show the decision_function scores for first 20 instances\n",
    "y_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91efe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "y_proba_lr = lr.fit(X_train, y_train).predict_proba(X_test)\n",
    "y_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1]))\n",
    "\n",
    "# show the probability of positive class for first 20 instances\n",
    "y_proba_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a98e",
   "metadata": {},
   "source": [
    "### Precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5dd539",
   "metadata": {},
   "source": [
    "### ROC curves, Area-Under-Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c98c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "y_score_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_score_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f338c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "for g in [0.01, 0.1, 0.20, 1]:\n",
    "    svm = SVC(gamma=g).fit(X_train, y_train)\n",
    "    y_score_svm = svm.decision_function(X_test)\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    accuracy_svm = svm.score(X_test, y_test)\n",
    "    print(\"gamma = {:.2f}  accuracy = {:.2f}   AUC = {:.2f}\".format(g, accuracy_svm, \n",
    "                                                                    roc_auc_svm))\n",
    "    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7, \n",
    "             label='SVM (gamma = {:0.2f}, area = {:0.2f})'.format(g, roc_auc_svm))\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "plt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.title('ROC curve: (1-of-10 digits classifier)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27745589",
   "metadata": {},
   "source": [
    "### Evaluation measures for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aafa8a",
   "metadata": {},
   "source": [
    "#### Multi-class confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95971274",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, \n",
    "                     index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                       svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "svm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, index = [i for i in range(0,10)],\n",
    "                  columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize = (5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM RBF Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                    svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19536f2c",
   "metadata": {},
   "source": [
    "#### Multi-class classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14112f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405425e",
   "metadata": {},
   "source": [
    "#### Micro- vs. macro-averaged metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Micro-averaged precision = {:.2f} (treat instances equally)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged precision = {:.2f} (treat classes equally)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572e148",
   "metadata": {},
   "source": [
    "### Regression evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data[:, None, 6]\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n",
    "\n",
    "y_predict = lm.predict(X_test)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n",
    "\n",
    "print('Linear model, coefficients: ', lm.coef_)\n",
    "print(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(y_test, \n",
    "                                                                     y_predict_dummy_mean)))\n",
    "print(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\n",
    "print(\"r2_score (dummy): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_predict, color='green', linewidth=2)\n",
    "plt.plot(X_test, y_predict_dummy_mean, color='red', linestyle = 'dashed', \n",
    "         linewidth=2, label = 'dummy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592dae1",
   "metadata": {},
   "source": [
    "### Model selection using evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e66f0",
   "metadata": {},
   "source": [
    "#### Cross-validation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "# again, making this a binary problem with 'digit 1' as positive class \n",
    "# and 'not 1' as negative class\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "# accuracy is the default scoring metric\n",
    "print('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=5))\n",
    "# use AUC as scoring metric\n",
    "print('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=5, scoring = 'roc_auc'))\n",
    "# use recall as scoring metric\n",
    "print('Cross-validation (recall)', cross_val_score(clf, X, y, cv=5, scoring = 'recall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb11aef",
   "metadata": {},
   "source": [
    "#### Grid search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "grid_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n",
    "\n",
    "# default metric to optimize over grid parameters: accuracy\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n",
    "\n",
    "print('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_clf_acc.best_score_)\n",
    "\n",
    "# alternative metric to optimize over grid parameters: AUC\n",
    "grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\n",
    "grid_clf_auc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test) \n",
    "\n",
    "print('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))\n",
    "print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\n",
    "print('Grid best score (AUC): ', grid_clf_auc.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd660b",
   "metadata": {},
   "source": [
    "#### Evaluation metrics supported for model selection   \n",
    "<img src=\"data/metrics.png\" />   \n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344ad11",
   "metadata": {},
   "source": [
    "### Two-feature classification example using the digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f276c",
   "metadata": {},
   "source": [
    "#### Optimizing a classifier using different evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f675a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create a two-feature input vector matching the example plot above\n",
    "# We jitter the points (add a small amount of random noise) in case there are areas\n",
    "# in feature space where many instances have the same features.\n",
    "jitter_delta = 0.25\n",
    "X_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\n",
    "X_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n",
    "\n",
    "clf = SVC(kernel = 'linear').fit(X_twovar_train, y_train)\n",
    "grid_values = {'class_weight':['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}]}\n",
    "plt.figure(figsize=(9,6))\n",
    "for i, eval_metric in enumerate(('precision','recall', 'f1','roc_auc')):\n",
    "    grid_clf_custom = GridSearchCV(clf, param_grid=grid_values, scoring=eval_metric)\n",
    "    grid_clf_custom.fit(X_twovar_train, y_train)\n",
    "    print('Grid best parameter (max. {0}): {1}'\n",
    "          .format(eval_metric, grid_clf_custom.best_params_))\n",
    "    print('Grid best score ({0}): {1}'\n",
    "          .format(eval_metric, grid_clf_custom.best_score_))\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plot_class_regions_for_classifier_subplot(grid_clf_custom, X_twovar_test, y_test, None,\n",
    "                                             None, None,  plt.subplot(2, 2, i+1))\n",
    "    \n",
    "    plt.title(eval_metric+'-oriented SVC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80dff1b",
   "metadata": {},
   "source": [
    "#### Precision-recall curve for the default SVC classifier (with balanced class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee007bd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# create a two-feature input vector matching the example plot above\n",
    "jitter_delta = 0.25\n",
    "X_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\n",
    "X_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n",
    "\n",
    "clf = SVC(kernel='linear', class_weight='balanced').fit(X_twovar_train, y_train)\n",
    "\n",
    "y_scores = clf.decision_function(X_twovar_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plot_class_regions_for_classifier(clf, X_twovar_test, y_test, \n",
    "                                  title=\"SVC, class_weight = 'balanced', optimized for accuracy\")\n",
    "\n",
    "plt.figure()\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.title (\"Precision-recall curve: SVC, class_weight = 'balanced'\")\n",
    "plt.plot(precision, recall, label = 'Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize=12, fillstyle='none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.show()\n",
    "print('At zero threshold, precision: {:.2f}, recall: {:.2f}'\n",
    "      .format(closest_zero_p, closest_zero_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491445a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed695d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THESE FIRST.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918ea20",
   "metadata": {},
   "source": [
    "## Practice Tasks\n",
    "\n",
    "In the following practice tasks you will train several models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    " \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `Class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c6451",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Import the data from `data/fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>There is a pandas method to create unique value counts for a Series object.</li>\n",
    "        <li>Every shape attribute is a tuple that one can access with fundamental python indexing.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert answer_one() == 356/21693, \"Your results seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following tasks.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1462896",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_one():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;df = pd.read_csv(\"data/fraud_data.csv\")</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return df[\"Class\"].value_counts().loc[1.0]/df.shape[0]</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>DummyClassifier's score method yields the accuracy score for a user-given X and y.</li>\n",
    "        <li>The recall_score method takes the actual and predicted y-values as parameters.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1aab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "assert len(answer_two()) == 2, \"Please return a tuple of length two!\"\n",
    "assert answer_two()[0] == 0.9852507374631269, \"Your accuracy score seems to be incorrect!\"\n",
    "assert answer_two()[1] == 0, \"Your recall score seems to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bfbc2",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_two():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.dummy import DummyClassifier</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.metrics import recall_score</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;model = DummyClassifier().fit(X_train, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;predictions = model.predict(X_test)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return model.score(X_test, y_test), recall_score(y_test, predictions)</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 3\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, `y_test` (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*   \n",
    "\n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>SVC's score method yields the accuracy score for a user-given X and y.</li>\n",
    "        <li>The recall_score and precision_score methods take the actual and predicted y-values as parameters.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be252b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "results = answer_three()\n",
    "assert len(results) == 3, \"Please return a tuple of length three!\"\n",
    "assert results[0] == 0.9900442477876106, \"Your accuracy score seems to be incorrect!\"\n",
    "assert results[1] == 0.35, \"Your recall score seems to be incorrect!\"\n",
    "assert results[2] == 0.9333333333333333, \"Your precision score seems to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca63af2",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_three():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.metrics import recall_score, precision_score</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.svm import SVC</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;model = SVC().fit(X_train, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;predictions = model.predict(X_test)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return model.score(X_test, y_test), recall_score(y_test, predictions), precision_score(y_test, predictions)</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function? Use `X_test` and `y_test`.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>SVC's decision_function method returns values to which a threshold is applicable.</li>\n",
    "        <li>Remind yourself of boolean masking with numpy arrays.</li>\n",
    "        <li>Python handles boolean values the same as integers/floats 0 (False) and 1 (True).</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWERS HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22573cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "results = answer_four()\n",
    "assert results.shape == (2, 2), \"Please return a 2x2 numpy-array!\"\n",
    "assert results[0, 0] == 5320, \"Your results seem to be incorrect!\"\n",
    "assert results[-1, -1] == 66, \"Your results seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8ca1a",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_four():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.metrics import confusion_matrix</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.svm import SVC</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;model = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;predictions = model.decision_function(X_test) > -220</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return confusion_matrix(y_test, predictions)</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 5\n",
    "\n",
    "Train a logisitic regression classifier (with `solver=\"liblinear\"`) using `X_train` and `y_train`.\n",
    "\n",
    "For the logisitic regression classifier, create a plot with two subplots that features a precision recall curve and a roc curve using `y_test` and the probability estimates for `X_test` (probability it is fraud).   \n",
    "   \n",
    "Your plot should look like this:  <img src=\"data/precision_recall_roc.png\">\n",
    "\n",
    "*You should create the plot outside of the function. The function should return a tuple with four arrays, i.e. `(precision_values [shape = (3253,)], recall_values [shape = (3253,)], false_positive_rates [shape = (50,)], true_positive_rates [shape = (50,)])`.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>Sklearn's precision_recall_curve method takes actual and predicted y-values as inputs and returns three arrays in the following order: precision, recall, and thresholds.</li>\n",
    "        <li>Sklearn's roc curve takes actual and predicted y-values as inputs and returns three arrays in the following order: false-positive rates, true-positive rates, and thresholds.</li>\n",
    "        <li>To create a subplot in matplotlib, one needs to call plt.subplot(rows, columns, id). In our case: plt.subplot(2,1,1) and plt.subplot(2,1,2).</li>\n",
    "        <li>To enhance your visualization: The method plt.tight_layout() takes care of the padding between and around subplots.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "# TRAIN YOUR MODEL HERE.\n",
    "\n",
    "\n",
    "# CREATE PLOT HERE.\n",
    "\n",
    "\n",
    "def answer_five():\n",
    "    \n",
    "    return # RETURN THE DESIRED ARRAYS HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73223b20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "results = answer_five()\n",
    "assert results[0].shape and results[1].shape == (3253,), \"Please return the arrays in the right order!\"\n",
    "assert results[2].shape and results[2].shape == (50,), \"Please return the arrays in the right order!\"\n",
    "assert results[0][500] == 0.028291621327529923, \"Your results seem to be incorrect!\"\n",
    "assert results[1][600] == 0.975, \"Your results seem to be incorrect!\"\n",
    "assert results[2][30] == 0.20527694610778444, \"Your results seem to be incorrect!\"\n",
    "assert results[3][25] == 0.925, \"Your results seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7cba2",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>from sklearn.linear_model import LogisticRegression</code><br />\n",
    "    <code>from sklearn.metrics import precision_recall_curve, roc_curve</code><br />\n",
    "    <code>import matplotlib.pyplot as plt</code><br />\n",
    "    <code></code><br />\n",
    "    <code>lr = LogisticRegression(solver=\"liblinear\")</code><br />\n",
    "    <code>lr.fit(X_train, y_train)</code><br />\n",
    "    <code>predictions = lr.decision_function(X_test)</code><br />\n",
    "    <code></code><br />\n",
    "    <code>fig = plt.figure()</code><br />\n",
    "    <code>ax1 = fig.add_subplot(2, 1, 1)</code><br />\n",
    "    <code>precision, recall, thresholds_pr = precision_recall_curve(y_test, predictions)</code><br />\n",
    "    <code>ax1.plot(precision, recall)</code><br />\n",
    "    <code>ax1.set_title(\"Precision-recall curve\")</code><br />\n",
    "    <code>ax1.set_xlabel(\"Precision\")</code><br />\n",
    "    <code>ax1.set_ylabel(\"Recall\")</code><br />\n",
    "    <code></code><br />\n",
    "    <code>ax2 = fig.add_subplot(2, 1, 2)</code><br />\n",
    "    <code>fpr, tpr, thresholds_roc = roc_curve(y_test, predictions)</code><br />\n",
    "    <code>ax2.plot(fpr, tpr)</code><br />\n",
    "    <code>ax2.set_title(\"ROC\")</code><br />\n",
    "    <code>ax2.set_xlabel(\"False positive rate\")</code><br />\n",
    "    <code>ax2.set_ylabel(\"True positive rate\")</code><br />\n",
    "    <code></code><br />\n",
    "    <code>fig.tight_layout()</code><br />\n",
    "    <code></code><br />\n",
    "    <code>def answer_five():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return precision, recall, fpr, tpr</code><br />\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "### Task 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier (with `solver=\"liblinear\"`), using `X_train` and `X_test`, recall for scoring, as well as the default 5-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: Do not return a DataFrame, just the values denoted by '?' above in a numpy array. You might need to reshape your raw result to meet the format we are looking for.*   \n",
    "<br />\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"red\"><b>Hints (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>The parameter grid needs to be a dictionary with parameter names as its keys and lists as its values. The different lists' elements correspond to respective parameter settings.</li>\n",
    "        <li>cv_results_ returns a dictionary. Its key \"mean_test_score\" yields the desired test scores.</li>\n",
    "        <li>The grid-search is performed as follows: [(\"l1\", 0.01), (\"l2\", 0.01), (\"l1\", 0.1), (\"l2\", 0.1), ...]</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # START YOUR CODE HERE.\n",
    "    \n",
    "    return # RETURN YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TESTS YOUR RESULTS.\n",
    "results = answer_six()\n",
    "assert results.shape == (5, 2), \"Your results are in the wrong format!\"\n",
    "assert results[2, 1] == 0.8114935064935065, \"Your results seem to be incorrect!\"\n",
    "assert results[-1, -1] == 0.8006493506493506, \"Your results seem to be incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce311460",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution (click to expand)</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <code>def answer_six():</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.model_selection import GridSearchCV</code><br /> \n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;from sklearn.linear_model import LogisticRegression</code><br />  \n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;param_grid = {</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"penalty\": [\"l1\", \"l2\"],</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"C\": [0.01, 0.1, 1, 10, 100]</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;}</code><br />\n",
    "    <code></code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;clf = GridSearchCV(estimator=LogisticRegression(solver=\"liblinear\"),</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;param_grid=param_grid, scoring=\"recall\")</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;clf.fit(X_train, y_train)</code><br />\n",
    "    <code>&nbsp;&nbsp;&nbsp;&nbsp;return clf.cv_results_[\"mean_test_score\"].reshape(5, 2)</code><br />\n",
    "</p>\n",
    "</details> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
